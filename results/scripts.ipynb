{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pymystem3 ruwordnet pyaspeller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ruwordnet download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pymystem3\n",
    "\n",
    "from ruwordnet import RuWordNet\n",
    "\n",
    "from pyaspeller import YandexSpeller\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from sys import setrecursionlimit\n",
    "setrecursionlimit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spellchecker():\n",
    "\n",
    "  speller: object\n",
    "  spell: object\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    pass\n",
    "\n",
    "  def fix_sent(self, s, w):\n",
    "    try:\n",
    "      return ' '.join(self.spell(s+' '+w).split()[len(s.split()):]).lower()\n",
    "    except:\n",
    "      return w\n",
    "\n",
    "  def fix_tuned(self, s, w):\n",
    "    word = self.spell(w)\n",
    "    sent = ' '.join(self.spell(s+' '+w).split()[len(s.split()):])\n",
    "    w_vars = next(self.speller.spell(w), {'s':[]})['s']\n",
    "    s_vars = [x for x in self.speller.spell(s+' '+w) if x['word']==w]\n",
    "    if s_vars:\n",
    "      s_vars = s_vars[-1]['s']\n",
    "\n",
    "    if word!=w or sent!=w:\n",
    "      if w in w_vars+s_vars:\n",
    "        return w\n",
    "      elif sent!=w:\n",
    "        return sent\n",
    "      else:\n",
    "        return word\n",
    "    else:\n",
    "      return w\n",
    "\n",
    "class YaSpeller(Spellchecker):\n",
    "  def __init__(self, max_requests=100):\n",
    "    self.speller = YandexSpeller(lang='ru', ignore_capitalization=True, max_requests=max_requests)\n",
    "    self.spell = self.speller.spelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictabilityDataProcessing:\n",
    "  def __init__(self):\n",
    "    self.wn = RuWordNet()\n",
    "    self.morph = pymystem3.Mystem(disambiguation=True)\n",
    "    self.speller = YaSpeller()\n",
    "    self.preprocess_cache = dict()\n",
    "    self.sem_cache = dict()\n",
    "    self.rel_cache = dict()\n",
    "    self.spell_cache = dict()\n",
    "  \n",
    "  def analyse_grammar(self, data):\n",
    "    items = data.replace(to_replace=np.nan, value='')['item']\n",
    "    texts = '\\n'.join([(i.strip()+' '+a).strip() if a else (i.strip()+' DUMMY').strip() for i, a in zip(items, data.answer)])+'\\n'\n",
    "    \n",
    "    analyses = np.array(self.morph.analyze(texts))\n",
    "    n_indexes = np.where((analyses == {'text': '\\n'}) | (analyses == {'text': '-\\n'}))[0]\n",
    "    words = [analyses[i-1] for i in n_indexes]\n",
    "\n",
    "    print('Collecting lemmas...')\n",
    "    lemma = [word['analysis'][0]['lex'] if ('analysis' in word and word['analysis']) else '' for word in words]\n",
    "\n",
    "    print('Collecting POS tags...')\n",
    "    pos = [word['analysis'][0]['gr'].split('=')[0].split(',')[0] if ('analysis' in word and word['analysis']) else '' for word in words]\n",
    "    \n",
    "    print('Collecting grammatical features...')\n",
    "    gram = []\n",
    "    for word in words:\n",
    "      if ('analysis' in word and word['analysis']):\n",
    "        try:\n",
    "          gram.append(word['analysis'][0]['gr'].split('=')[0].split(',')[1]+','+word['analysis'][0]['gr'].split('=')[1])\n",
    "        except:\n",
    "          gram.append(word['analysis'][0]['gr'].split('=')[1])\n",
    "      else:\n",
    "        gram.append('')\n",
    "    \n",
    "    gram = ['|'.join( [g.split('(')[0]+x.strip(')') for x in g.split('(')[1].split('|')] ) if '(' in g else g for g in gram]\n",
    "    \n",
    "    return {'lemma': lemma, 'pos': pos, 'gram': gram}\n",
    "  \n",
    "  def analyse_sem(self, lemma):\n",
    "    sem = []\n",
    "    \n",
    "    for lem in tqdm(lemma):\n",
    "      if lem:\n",
    "        if lem in self.sem_cache:\n",
    "          sem.append(self.sem_cache[lem])\n",
    "        else:\n",
    "          try:\n",
    "            sem.append({chain.split('>')[0].lower(): chain.lower() for chain \n",
    "                        in self.__printer__(self.__get_hypernyms_chains__(lem)) if chain})\n",
    "            self.sem_cache[lem] = sem[-1]\n",
    "          except:\n",
    "            sem.append('')\n",
    "            self.sem_cache[lem] = ''\n",
    "      else:\n",
    "        sem.append('')\n",
    "    \n",
    "    return [s if s else '' for s in sem]\n",
    "\n",
    "  def analyse_rel(self, lemma):\n",
    "    rel = []\n",
    "    \n",
    "    for lem in tqdm(lemma):\n",
    "      if lem:\n",
    "        if lem in self.rel_cache:\n",
    "          rel.append(self.rel_cache[lem])\n",
    "        else:\n",
    "          rel.append(self.__other_sem__(lem))\n",
    "          self.rel_cache[lem] = rel[-1]\n",
    "      else:\n",
    "        rel.append('')\n",
    "    \n",
    "    return rel\n",
    "\n",
    "  def fix_spelling(self, line):\n",
    "    if line.rel:\n",
    "      return line.answer\n",
    "    else:\n",
    "      if int(line['number'])>1:\n",
    "        s = line['item'].strip()\n",
    "      else:\n",
    "        s = ''\n",
    "      if isinstance(line.answer, str):\n",
    "        if s+' '+line.answer in self.spell_cache:\n",
    "          return self.spell_cache[s+' '+line.answer]\n",
    "        else:\n",
    "          sp = self.speller.fix_tuned(s, line.answer)\n",
    "          self.spell_cache[s+' '+line.answer] = sp\n",
    "          return sp\n",
    "      else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "  def process_data(self, data_to_process, process_answers=True, process_grammar=True, \n",
    "                   process_sem=True, process_rel=True, process_spelling=True):\n",
    "    \n",
    "    data = data_to_process.copy()\n",
    "\n",
    "    if process_answers:\n",
    "      print('Preprocessing data...')\n",
    "      if 'raw_answer' not in data:\n",
    "        data['raw_answer'] = data['answer']\n",
    "      answers = data.progress_apply(self.preprocessing, axis=1)\n",
    "      data['answer'] = answers\n",
    "    \n",
    "    if process_grammar:\n",
    "      print('Analysing grammar...')\n",
    "      grammar = self.analyse_grammar(data)\n",
    "      data['lemma'] = grammar['lemma']\n",
    "      data['pos'] = grammar['pos']\n",
    "      data['gram'] = grammar['gram']\n",
    "\n",
    "    if process_sem:\n",
    "      print('Collecting semantic chains...')\n",
    "      sems = self.analyse_sem(data['lemma'])\n",
    "      data['sems'] = sems\n",
    "    \n",
    "    if process_rel:\n",
    "      print('Collecting other semantic relations...')\n",
    "      rel = self.analyse_rel(data['lemma'])\n",
    "      data['rel'] = rel\n",
    "\n",
    "    if process_spelling:\n",
    "      print('Respelling...')\n",
    "      spellings = data.progress_apply(self.fix_spelling, axis=1)\n",
    "      data['processed_answer'] = data['answer']\n",
    "      data['answer'] = spellings\n",
    "      data_same = data[data.answer==data.processed_answer]\n",
    "      data_corrected = data[data.answer != data.processed_answer]\n",
    "      if not data_corrected.empty:\n",
    "        print('Reanalysing corrected data...')\n",
    "        data_corrected = self.process_data(data_corrected, process_answers=False, process_spelling=False)\n",
    "        return pd.concat([data_same, data_corrected])\n",
    "    \n",
    "    return data\n",
    "\n",
    "  def __get_hypernyms_chain__(self, synset):\n",
    "    hypernyms = {hyper:{} for hyper in synset.hypernyms}\n",
    "    if hypernyms:\n",
    "      for hyper in hypernyms:\n",
    "        hypernyms[hyper] = self.__get_hypernyms_chain__(hyper)\n",
    "    return hypernyms\n",
    "  \n",
    "  def __printer__(self, razbor):\n",
    "    razbor_p = []\n",
    "    if not razbor:\n",
    "        return ['']\n",
    "    else:\n",
    "        for x in razbor:\n",
    "            razbor_p += [x.title+'>'+stroka for stroka in self.__printer__(razbor[x])]\n",
    "    return razbor_p\n",
    "  \n",
    "  def __get_hypernyms_chains__(self, word):\n",
    "    try:\n",
    "      synsets = {homo.synset:{} for homo in self.wn[word]}\n",
    "    except KeyError:\n",
    "      return ''\n",
    "    for synset in synsets:\n",
    "      synsets[synset] = self.__get_hypernyms_chain__(synset)\n",
    "    return synsets\n",
    "  \n",
    "  def __other_sem__(self, word):\n",
    "    try:\n",
    "      synsets = {homo.synset:{} for homo in self.wn[word]}\n",
    "    except KeyError:\n",
    "      return ''\n",
    "    for synset in synsets:\n",
    "      synsets[synset] = {\n",
    "          'antonyms':[x.title.lower() for x in synset.antonyms],\n",
    "          'domains':[x.title.lower() for x in synset.domains],\n",
    "          'domain_items':[x.title.lower() for x in synset.domain_items],\n",
    "          'meronyms':[x.title.lower() for x in synset.meronyms],\n",
    "          'holonyms':[x.title.lower() for x in synset.holonyms],\n",
    "          'premises':[x.title.lower() for x in synset.premises],\n",
    "          'conclusions':[x.title.lower() for x in synset.conclusions],\n",
    "          'causes':[x.title.lower() for x in synset.causes],\n",
    "          'effects':[x.title.lower() for x in synset.effects],\n",
    "          'related':[x.title.lower() for x in synset.related]\n",
    "          }\n",
    "    return {k.title.lower(): v for k, v in synsets.items()}\n",
    "  \n",
    "  def preprocessing(self, line):\n",
    "\n",
    "    if isinstance(line['raw_answer'], str):\n",
    "\n",
    "      if int(line['number'])>1:\n",
    "          stimul = str(line['item'])+' '+str(line.raw_answer).lower()\n",
    "      else:\n",
    "          stimul = str(line.raw_answer).lower()\n",
    "\n",
    "      if stimul in self.preprocess_cache:\n",
    "        return self.preprocess_cache[stimul]\n",
    "\n",
    "      if isinstance(line['item'], str):\n",
    "        c = re.sub('[^а-яА-ЯёЁ\\-\\s]', '', line['item'])\n",
    "        i = c.lower().split()\n",
    "      else:\n",
    "        c = ''\n",
    "        i = ['введите', 'первое', 'слово']\n",
    "\n",
    "      s = re.sub('[^а-яА-ЯёЁ\\-\\s]', '', line['raw_answer'])\n",
    "      s = re.sub('\\s+', ' ', s)\n",
    "      s = s.strip().lower()\n",
    "\n",
    "      if s == c:\n",
    "        return ''\n",
    "\n",
    "      if ' ' in s:\n",
    "        if not sum([re.sub('ё', 'е', x)!=re.sub('ё', 'е', y) for x, y in zip(s.split(), i)]):\n",
    "          s = ' '.join(s.split()[len(i):])\n",
    "\n",
    "        if ' ' in s:\n",
    "          ns = self.speller.fix_tuned(c, ' '.join(s.split()[:2]))\n",
    "          if ' ' in ns:\n",
    "            s = s.split()[0]\n",
    "          else:\n",
    "            s = ns\n",
    "\n",
    "      self.preprocess_cache[stimul] = s.strip()\n",
    "\n",
    "      return s.strip()\n",
    "    \n",
    "    else:\n",
    "      return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccuracyTables():\n",
    "  \n",
    "    def __init__(self, original):\n",
    "\n",
    "        self.original = original\n",
    "    \n",
    "    def process_data(self, data):\n",
    "\n",
    "        table = pd.merge(left=data, right=self.original, how='left', on=['original', 'number'],\n",
    "               suffixes=('_given', '_true'))\n",
    "\n",
    "        table = table.replace(np.nan, '')\n",
    "\n",
    "        table[['lex_accuracy', 'lemma_accuracy', 'pos_accuracy', 'sem_accuracy', 'gram_accuracy']] = table.progress_apply(self.get_accuracies, axis=1, result_type='expand')\n",
    "\n",
    "        table = table.drop(columns=['item_true'])\n",
    "\n",
    "        return table\n",
    "\n",
    "    def get_accuracies(self, line):\n",
    "\n",
    "        if line.answer_given == line.answer_true:\n",
    "            lex = 1\n",
    "        else:\n",
    "            lex = 0\n",
    "\n",
    "        if line.pos_given==line.pos_true:\n",
    "            if line.pos_given!='':\n",
    "                pos=1\n",
    "            else:\n",
    "                pos=np.nan\n",
    "        else:\n",
    "            pos=0\n",
    "\n",
    "        if line.lemma_given==line.lemma_true:\n",
    "            if line.lemma_given!='':\n",
    "                lemma=1\n",
    "            else:\n",
    "                lemma=np.nan\n",
    "        else:\n",
    "            lemma=0\n",
    "\n",
    "        if line.sems_true and line.sems_given:\n",
    "            sems_given = [set([sem for sem in s.split('>') if sem]) for s in line.sems_given.values()]\n",
    "            sems_true = [set([sem for sem in s.split('>') if sem]) for s in line.sems_true.values()]\n",
    "\n",
    "            sem_inter = []\n",
    "            for g in sems_given:\n",
    "                for t in sems_true:\n",
    "                    sem_inter.append(len(g&t)/len(t))\n",
    "            sem = max(sem_inter)\n",
    "        else:\n",
    "            sem = np.nan\n",
    "\n",
    "        if line.gram_true and line.sems_given:\n",
    "            gram_given = [set([gram for gram in g.split(',') if gram]) for g in line.gram_given.split('|')]\n",
    "            gram_true = [set([gram for gram in g.split(',') if gram]) for g in line.gram_true.split('|')]\n",
    "\n",
    "            gram_inter = []\n",
    "            for g in gram_given:\n",
    "                for t in gram_true:\n",
    "                    gram_inter.append(len(g&t)/len(t))\n",
    "                gram = max(gram_inter)\n",
    "        else:\n",
    "            gram = np.nan\n",
    "\n",
    "        return lex, lemma, pos, sem, gram"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
